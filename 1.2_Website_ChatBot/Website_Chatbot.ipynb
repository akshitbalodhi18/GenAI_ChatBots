{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a very basic chatbot that gives you answer to your queries from any link that you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | MediumOpen in appSign upSign inWriteSign upSign inEmbedding Models Explained: A Guide to NLP’s Core TechnologyNayab Hassan·Follow12 min read·Aug 16, 2024--ListenShareIntroductionEmbedding models have revolutionized machine learning by enabling the transformation of complex data into lower-dimensional representations that are easier to process. These models have been particularly impactful in fields such as natural language processing (NLP), computer vision, and recommendation systems. Embedding refers to mapping high-dimensional data (e.g., text, images) into dense, lower-dimensional vectors that preserve semantic relationships.Understanding embedding models is key to unlocking the power of modern AI systems. This guide will take you through the fundamentals of embedding models, explore recent advancements like BERT and GPT, and provide real-world examples and best practices. By the end of this guide, you’ll have a deep understanding of embedding models, their applications, and how they can be implemented effectively.Understanding Embedding ModelsWhat Are Embeddings?At the core of embedding models is the concept of **embedding**, which refers to representing high-dimensional data as vectors in a lower-dimensional space. This transformation is critical because it enables machine learning algorithms to process and understand complex inputs, such as words, sentences, images, and even graphs.For example, words like “king” and “queen” can be represented as vectors that are close to each other in the vector space, reflecting their semantic similarity. This is in contrast to one-hot encoding, where each word is represented as a sparse binary vector, which fails to capture any semantic relationships.Embeddings are learned through various machine learning techniques, and the resulting vectors can be used in downstream tasks such as classification, clustering, and recommendation.Types of Embeddings1._Word Embeddings: These embeddings represent individual words as vectors. Examples include Word2Vec, GloVe, and FastText. Word embeddings are foundational in NLP tasks and enable models to understand the semantic relationships between words.Example: In Word2Vec, a word like “king” might be represented by a vector such as `[0.5, 0.8, -0.1, 0.3, 0.9]`. The word “queen” would have a similar vector, showing their semantic similarity, while a word like “apple” would be more distant in the vector space.2. Sentence Embeddings: These embeddings capture the meaning of entire sentences or paragraphs. Models like the Universal Sentence Encoder and Sentence-BERT (SBERT) generate these embeddings by averaging or pooling word embeddings to create a single vector for the entire sentence.Example: In a sentence like “The cat sat on the mat,” sentence embeddings might map this entire sentence to a vector that encapsulates the overall meaning, rather than just the individual words.Image Embeddings: These embeddings are used in computer vision to represent images as vectors. Convolutional neural networks (CNNs) often serve as the backbone for generating these embeddings, which can then be used for tasks such as image retrieval or classification.Example: An image of a cat might be mapped to a vector that is close to other images of cats in the embedding space, helping the model recognize similar objects across different images.4. Graph Embeddings: In graph embeddings, nodes or entire sub graphs are mapped to vectors that preserve the structural relationships within the graph. Models like DeepWalk and GraphSAGE are used to generate these embeddings.Example: In a social network graph, embeddings can be used to represent users as vectors, where the proximity of vectors indicates the strength of relationships between users.Different Vector EmbeddingsKey Embedding ModelsWord2VecWord2Vec, developed by Google, was one of the pioneering models in word embeddings. It uses shallow neural networks to learn word associations from a large corpus of text. Word2Vec has two primary approaches: Continuous Bag of Words (CBOW) and Skip-Gram.CBOW: In CBOW, the model predicts a target word given its surrounding context words. For example, in the sentence “The cat sat on the mat,” the model might predict “sat” given the context words “cat,” “on,” and “the.”Skip-Gram: The Skip-Gram model works in reverse, predicting the context words given a target word. For example, given the word “sat,” the model would predict “cat,” “on,” and “the.”Skip-Gram ArchitectureWord2Vec learns to generate embeddings where semantically similar words have similar vector representations.Example: A famous example involves the analogy: “king” — “man” + “woman” ≈ “queen.” This demonstrates that Word2Vec captures both the semantic meaning and relationships between words.GloVe (Global Vectors for Word Representation)GloVe, developed by Stanford University, is another popular model for word embeddings. Unlike Word2Vec, which is prediction-based, GloVe is a count-based model that leverages the co-occurrence matrix of words across the entire corpus. GloVe aims to create word vectors that capture the global statistical information of word co-occurrences.Advantage: GloVe performs well in capturing both local and global context, making it suitable for various NLP tasks.Example: If words like “ice” and “snow” frequently co-occur in a corpus, GloVe will create vectors where “ice” and “snow” are close in the vector space, reflecting their semantic relationship.Framework for Constructing GloVeBERT (Bidirectional Encoder Representations from Transformers)BERT represents a significant advancement in embedding models. Unlike traditional models that treat words independently, BERT generates contextualized embeddings, meaning the same word can have different vectors depending on its context. BERT is based on the Transformer architecture, which uses attention mechanisms to model relationships between words in a sentence.Key Feature: BERT is bidirectional, meaning it looks at both the left and right context of a word during training. This allows BERT to capture the full meaning of a word within a sentence, unlike models that process words in a unidirectional manner.Architecture: BERT uses multiple layers of Transformers to create embeddings. Each Transformer layer applies self-attention mechanisms to capture relationships between words, regardless of their distance in the sentence.Training: BERT is trained using two tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). In MLM, some words in the input sentence are masked, and the model learns to predict the missing words. In NSP, the model learns to predict whether two sentences are sequentially related.Application: BERT excels in tasks like question answering and text classification. For example, in a customer support system, BERT can be used to understand and respond to customer queries with high accuracy.Example: In the sentence “The bank can guarantee the deposit will arrive tomorrow,” BERT will generate different embeddings for the word “bank” based on its surrounding context. It understands whether “bank” refers to a financial institution or the side of a river, depending on the context.Bert ArchitectureGPT (Generative Pre-trained Transformer)GPT, developed by OpenAI, is another transformer-based model but focuses on generative tasks. Unlike BERT, which is designed primarily for understanding text, GPT excels at generating coherent text based on a given prompt. GPT uses a unidirectional transformer, meaning it only considers the left-side context when generating text.Key Feature: GPT is pre-trained on large amounts of text data and fine-tuned on specific tasks, making it highly versatile. Its ability to generate human-like text has made it popular in applications like chatbots, content generation, and creative writing.Architecture: GPT uses a stack of transformer decoder layers. Each layer applies self-attention to the input sequence and generates a prediction for the next token (word or subword) in the sequence.Training: GPT is pre-trained using an unsupervised learning approach, where the model learns to predict the next word in a sequence. Fine-tuning is then performed on specific tasks using supervised learning.Application: GPT has been used in various creative applications, such as generating articles, poems, and even code. For instance, GPT-3 can generate a complete essay based on a short prompt, with text that is often indistinguishable from human writing.Example: Given a prompt like “Once upon a time,” GPT can generate a full story, leveraging its understanding of language and context to create coherent and creative narratives.Real-World Applications of Embedding ModelsSentiment AnalysisEmbedding models are crucial in sentiment analysis, where they transform words into vectors that capture sentiment-related features. For example, words with positive sentiment like “happy” and “joyful” will have similar embeddings, helping the model detect sentiment in a text.Example: In a movie review, the sentence “The film was breathtaking and inspiring” can be represented as a sequence of embeddings. The sentiment analysis model processes these embeddings to classify the review as positive.Sentiment Analysis Pipeline DiagramRecommender SystemsRecommender systems leverage embedding models to represent both users and items (e.g., movies, products) as vectors. These embeddings are used to compute the similarity between users and items, enabling personalized recommendations.Example: In a movie recommendation system, embeddings can be used to represent both users and movies. If a user likes “Inception” and “Interstellar,” the model can recommend other movies with similar embeddings, such as “The Matrix.”Question Answering SystemsEmbedding models power question answering (QA) systems by mapping both questions and potential answers to a common vector space. This allows the model to retrieve the most relevant answer based on the similarity between the question and the answer embeddings.Example: In a QA system, a question like “What is the capital of France?” can be embedded as a vector. The model retrieves the most similar vector from a database of potential answers, returning “Paris.”Language TranslationIn machine translation, embedding models are used to represent words and sentences in different languages as vectors in a shared embedding space. This enables models to translate between languages by finding the closest matching vectors.Example: In a translation model, the English word “cat” and the Spanish word “gato” would have similar embeddings, allowing the model to translate between the two languages.Language Translation GPT architectureBest Practices and ConsiderationsChoosing the Right ModelTask-Specific Models: Choose the embedding model based on your specific task. For example, BERT is ideal for tasks requiring deep contextual understanding, while GPT is better suited for generative tasks.Pre-trained vs. Custom Models: Leveraging pre-trained models can save time and resources. However, fine-tuning or training custom models may be necessary for domain-specific tasks.Fine-Tuning for Domain-Specific TasksFine-tuning embedding models on domain-specific data can significantly improve performance. For instance, fine-tuning BERT on legal documents for a legal QA system will yield better results than using a general-purpose BERT model.Computational ConsiderationsEmbedding models, especially large ones like BERT and GPT, require substantial computational resources. It’s important to consider the trade-off between model size, training time, and inference speed when deploying embedding models in production systems.Ethical ConsiderationsEmbedding models can inadvertently capture biases present in the training data. It’s crucial to monitor and mitigate these biases to ensure that AI systems based on embedding models are fair and ethical.Example: A word embedding model might associate words like “nurse” more closely with female pronouns and “engineer” with male pronouns, reflecting societal biases. These biases must be addressed to prevent biased outcomes in AI applications.Recent Advancements in Embedding ModelsRoBERTa: A variant of BERT, RoBERTa optimizes training by removing the NSP task and training on larger datasets. It has demonstrated superior performance on various NLP benchmarks.GPT-4: The latest iteration of the GPT series, GPT-4, has further improved the model’s generative capabilities, making it more versatile in creative and complex text generation tasks.Sentence-BERT (SBERT): An extension of BERT, SBERT fine-tunes BERT for sentence similarity tasks by using Siamese and triplet networks. It produces embeddings that are more suitable for tasks like semantic search and clustering.Ethical ConsiderationsEmbedding models, despite their numerous applications, are not immune to ethical concerns. These models can capture and amplify biases present in the training data, leading to unfair or discriminatory outcomes in AI systems. Recognizing and mitigating these biases is critical to developing ethical AI solutions.Bias in Embeddings: A common ethical issue in embedding models is the presence of gender, racial, or cultural biases. For example, word embeddings trained on large text corpora may associate words like “doctor” with male pronouns and “nurse” with female pronouns, reflecting societal stereotypes. These biases can perpetuate discrimination when embedding models are deployed in real-world applications like hiring systems or recommendation algorithms.Mitigating Bias: To mitigate bias, developers can employ techniques such as de biasing embeddings, where biased dimensions in the embedding space are removed or neutralized. Regular audits of AI systems for bias and fairness, as well as using diverse and representative training data, can also help reduce bias in embedding models.Transparency and Accountability: Developers should also prioritize transparency and accountability in the deployment of embedding models. This includes making the training data and model decisions transparent to users, and ensuring there are mechanisms in place to address potential harms or biases that arise from the use of AI systems.Embedding models have transformative potential, but developers must approach their use with an ethical mindset, ensuring that AI systems serve all users fairly and responsibly.Embedding Models Usage in Large Language Models (LLMs)Embedding models form the backbone of large language models (LLMs) like GPT, BERT, and their variants. These models have gained prominence in various AI applications due to their ability to generate and understand human-like text. The success of LLMs is largely due to the use of sophisticated embedding techniques that transform text into meaningful vector representations.Tokenization and Embeddings:In LLMs, the first step is to tokenize the input text, breaking it down into words, subwords, or characters. Each token is then converted into an embedding, which serves as the input to the model. These embeddings capture the semantic meaning of the tokens and serve as the foundation for further processing in the model’s layers.Self-Attention Mechanism:LLMs rely heavily on the **self-attention mechanism**, which allows the model to focus on different parts of the input text when generating embeddings. This mechanism enables the model to capture long-range dependencies in the text, making LLMs particularly effective in tasks that require understanding context and relationships between words.Contextualized Embeddings:Unlike traditional word embeddings, which generate a fixed vector for each word, LLMs produce **contextualized embeddings**. This means that the same word can have different embeddings depending on the surrounding context. For example, the word “bat” will have different embeddings in the sentences “The bat flew at night” and “He swung the bat.”Transformer Architecture:The transformer architecture is central to LLMs. Transformers use layers of attention mechanisms to process the input text and generate contextualized embeddings. Each layer refines the embeddings based on the relationships between tokens, allowing the model to build a deep understanding of the input text. In models like GPT, this process is unidirectional, with the model generating text by predicting the next word in the sequence. In BERT, the process is bidirectional, allowing the model to consider both preceding and following text when generating embeddings.Transfer Learning in LLMs:One of the key advantages of LLMs is their ability to transfer knowledge learned from large-scale pre-training tasks to specific downstream tasks. During pre-training, the model learns to generate embeddings that capture general language patterns. Fine-tuning on specific tasks, such as sentiment analysis or text classification, allows the model to adapt these embeddings to the new task, often with minimal training data.Example: GPT-3, for instance, can be fine-tuned on a dataset of customer service conversations. The model will use its pre-trained embeddings to understand the context and generate appropriate responses, despite not being explicitly trained on customer service data during pretraining.Challenges with Embedding Models in LLMs:While embedding models in LLMs have advanced AI capabilities, they also present challenges. LLMs require vast amounts of computational resources for training and inference, making them expensive and energy-intensive. Additionally, the embeddings generated by LLMs are not always interpretable, posing challenges for transparency and trust in AI systems.Embedding models are integral to the success of LLMs, driving advancements in AI applications like text generation, translation, and question answering. As these models continue to evolve, embedding techniques will remain at the forefront of AI innovation.Additional ResourcesTo deepen your understanding of embedding models, consider the following resources:Courses“Deep Learning Specialization” by Andrew Ng on Coursera“Transformers for NLP” on UdemyBooks“Deep Learning with Python” by François Chollet“Natural Language Processing with Transformers” by Lewis Tunstall, Leandro von Werra, and Thomas WolfConclusionEmbedding models are the backbone of modern AI, enabling machines to understand and process complex data in ways that were previously impossible. From word embeddings like Word2Vec and GloVe to state-of-the-art models like BERT and GPT, embeddings have opened up new possibilities in fields such as NLP, computer vision, and recommendation systems.As these models continue to evolve, their impact on AI will only grow. By understanding and implementing embedding models, practitioners can build powerful AI systems that excel in tasks ranging from sentiment analysis to creative writing.By mastering embedding models, you’ll be well-equipped to tackle the challenges and opportunities of modern AI.NLPEmbedding ModelArtificial IntelligenceMachine LearningGui̇de----FollowWritten by Nayab Hassan28 Followers·20 FollowingHello there, I am an aspiring LLM Inference engineer who is also trying to set up his own game studio. Follow me as I discover different aspects of the IT.FollowNo responses yetHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Web Base Loader requires beautifulsoup4 to work so please import that module before\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | MediumOpen in appSign upSign inWriteSign upSign inEmbedding Models Explained: A Guide to NLP’s Core TechnologyNayab Hassan·Follow12 min read·Aug 16, 2024--ListenShareIntroductionEmbedding models have revolutionized machine learning by enabling the transformation of complex data into lower-dimensional representations that are easier to process. These models have been particularly impactful in fields such as natural language processing (NLP), computer vision, and recommendation systems. Embedding refers to mapping high-dimensional data (e.g., text, images) into dense, lower-dimensional vectors that preserve semantic relationships.Understanding embedding models is key to unlocking the power of modern AI systems. This guide will take you through the fundamentals of embedding models, explore recent advancements like BERT and GPT, and provide real-world examples and best practices. By the end of'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='AI systems. This guide will take you through the fundamentals of embedding models, explore recent advancements like BERT and GPT, and provide real-world examples and best practices. By the end of this guide, you’ll have a deep understanding of embedding models, their applications, and how they can be implemented effectively.Understanding Embedding ModelsWhat Are Embeddings?At the core of embedding models is the concept of **embedding**, which refers to representing high-dimensional data as vectors in a lower-dimensional space. This transformation is critical because it enables machine learning algorithms to process and understand complex inputs, such as words, sentences, images, and even graphs.For example, words like “king” and “queen” can be represented as vectors that are close to each other in the vector space, reflecting their semantic similarity. This is in contrast to one-hot encoding, where each word is represented as a sparse binary vector, which fails to capture any semantic'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='other in the vector space, reflecting their semantic similarity. This is in contrast to one-hot encoding, where each word is represented as a sparse binary vector, which fails to capture any semantic relationships.Embeddings are learned through various machine learning techniques, and the resulting vectors can be used in downstream tasks such as classification, clustering, and recommendation.Types of Embeddings1._Word Embeddings: These embeddings represent individual words as vectors. Examples include Word2Vec, GloVe, and FastText. Word embeddings are foundational in NLP tasks and enable models to understand the semantic relationships between words.Example: In Word2Vec, a word like “king” might be represented by a vector such as `[0.5, 0.8, -0.1, 0.3, 0.9]`. The word “queen” would have a similar vector, showing their semantic similarity, while a word like “apple” would be more distant in the vector space.2. Sentence Embeddings: These embeddings capture the meaning of entire sentences'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='similar vector, showing their semantic similarity, while a word like “apple” would be more distant in the vector space.2. Sentence Embeddings: These embeddings capture the meaning of entire sentences or paragraphs. Models like the Universal Sentence Encoder and Sentence-BERT (SBERT) generate these embeddings by averaging or pooling word embeddings to create a single vector for the entire sentence.Example: In a sentence like “The cat sat on the mat,” sentence embeddings might map this entire sentence to a vector that encapsulates the overall meaning, rather than just the individual words.Image Embeddings: These embeddings are used in computer vision to represent images as vectors. Convolutional neural networks (CNNs) often serve as the backbone for generating these embeddings, which can then be used for tasks such as image retrieval or classification.Example: An image of a cat might be mapped to a vector that is close to other images of cats in the embedding space, helping the model'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='then be used for tasks such as image retrieval or classification.Example: An image of a cat might be mapped to a vector that is close to other images of cats in the embedding space, helping the model recognize similar objects across different images.4. Graph Embeddings: In graph embeddings, nodes or entire sub graphs are mapped to vectors that preserve the structural relationships within the graph. Models like DeepWalk and GraphSAGE are used to generate these embeddings.Example: In a social network graph, embeddings can be used to represent users as vectors, where the proximity of vectors indicates the strength of relationships between users.Different Vector EmbeddingsKey Embedding ModelsWord2VecWord2Vec, developed by Google, was one of the pioneering models in word embeddings. It uses shallow neural networks to learn word associations from a large corpus of text. Word2Vec has two primary approaches: Continuous Bag of Words (CBOW) and Skip-Gram.CBOW: In CBOW, the model predicts a'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='shallow neural networks to learn word associations from a large corpus of text. Word2Vec has two primary approaches: Continuous Bag of Words (CBOW) and Skip-Gram.CBOW: In CBOW, the model predicts a target word given its surrounding context words. For example, in the sentence “The cat sat on the mat,” the model might predict “sat” given the context words “cat,” “on,” and “the.”Skip-Gram: The Skip-Gram model works in reverse, predicting the context words given a target word. For example, given the word “sat,” the model would predict “cat,” “on,” and “the.”Skip-Gram ArchitectureWord2Vec learns to generate embeddings where semantically similar words have similar vector representations.Example: A famous example involves the analogy: “king” — “man” + “woman” ≈ “queen.” This demonstrates that Word2Vec captures both the semantic meaning and relationships between words.GloVe (Global Vectors for Word Representation)GloVe, developed by Stanford University, is another popular model for word'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='Word2Vec captures both the semantic meaning and relationships between words.GloVe (Global Vectors for Word Representation)GloVe, developed by Stanford University, is another popular model for word embeddings. Unlike Word2Vec, which is prediction-based, GloVe is a count-based model that leverages the co-occurrence matrix of words across the entire corpus. GloVe aims to create word vectors that capture the global statistical information of word co-occurrences.Advantage: GloVe performs well in capturing both local and global context, making it suitable for various NLP tasks.Example: If words like “ice” and “snow” frequently co-occur in a corpus, GloVe will create vectors where “ice” and “snow” are close in the vector space, reflecting their semantic relationship.Framework for Constructing GloVeBERT (Bidirectional Encoder Representations from Transformers)BERT represents a significant advancement in embedding models. Unlike traditional models that treat words independently, BERT generates'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='(Bidirectional Encoder Representations from Transformers)BERT represents a significant advancement in embedding models. Unlike traditional models that treat words independently, BERT generates contextualized embeddings, meaning the same word can have different vectors depending on its context. BERT is based on the Transformer architecture, which uses attention mechanisms to model relationships between words in a sentence.Key Feature: BERT is bidirectional, meaning it looks at both the left and right context of a word during training. This allows BERT to capture the full meaning of a word within a sentence, unlike models that process words in a unidirectional manner.Architecture: BERT uses multiple layers of Transformers to create embeddings. Each Transformer layer applies self-attention mechanisms to capture relationships between words, regardless of their distance in the sentence.Training: BERT is trained using two tasks: Masked Language Modeling (MLM) and Next Sentence Prediction'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='mechanisms to capture relationships between words, regardless of their distance in the sentence.Training: BERT is trained using two tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). In MLM, some words in the input sentence are masked, and the model learns to predict the missing words. In NSP, the model learns to predict whether two sentences are sequentially related.Application: BERT excels in tasks like question answering and text classification. For example, in a customer support system, BERT can be used to understand and respond to customer queries with high accuracy.Example: In the sentence “The bank can guarantee the deposit will arrive tomorrow,” BERT will generate different embeddings for the word “bank” based on its surrounding context. It understands whether “bank” refers to a financial institution or the side of a river, depending on the context.Bert ArchitectureGPT (Generative Pre-trained Transformer)GPT, developed by OpenAI, is another'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='whether “bank” refers to a financial institution or the side of a river, depending on the context.Bert ArchitectureGPT (Generative Pre-trained Transformer)GPT, developed by OpenAI, is another transformer-based model but focuses on generative tasks. Unlike BERT, which is designed primarily for understanding text, GPT excels at generating coherent text based on a given prompt. GPT uses a unidirectional transformer, meaning it only considers the left-side context when generating text.Key Feature: GPT is pre-trained on large amounts of text data and fine-tuned on specific tasks, making it highly versatile. Its ability to generate human-like text has made it popular in applications like chatbots, content generation, and creative writing.Architecture: GPT uses a stack of transformer decoder layers. Each layer applies self-attention to the input sequence and generates a prediction for the next token (word or subword) in the sequence.Training: GPT is pre-trained using an unsupervised learning'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='Each layer applies self-attention to the input sequence and generates a prediction for the next token (word or subword) in the sequence.Training: GPT is pre-trained using an unsupervised learning approach, where the model learns to predict the next word in a sequence. Fine-tuning is then performed on specific tasks using supervised learning.Application: GPT has been used in various creative applications, such as generating articles, poems, and even code. For instance, GPT-3 can generate a complete essay based on a short prompt, with text that is often indistinguishable from human writing.Example: Given a prompt like “Once upon a time,” GPT can generate a full story, leveraging its understanding of language and context to create coherent and creative narratives.Real-World Applications of Embedding ModelsSentiment AnalysisEmbedding models are crucial in sentiment analysis, where they transform words into vectors that capture sentiment-related features. For example, words with positive'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='Embedding ModelsSentiment AnalysisEmbedding models are crucial in sentiment analysis, where they transform words into vectors that capture sentiment-related features. For example, words with positive sentiment like “happy” and “joyful” will have similar embeddings, helping the model detect sentiment in a text.Example: In a movie review, the sentence “The film was breathtaking and inspiring” can be represented as a sequence of embeddings. The sentiment analysis model processes these embeddings to classify the review as positive.Sentiment Analysis Pipeline DiagramRecommender SystemsRecommender systems leverage embedding models to represent both users and items (e.g., movies, products) as vectors. These embeddings are used to compute the similarity between users and items, enabling personalized recommendations.Example: In a movie recommendation system, embeddings can be used to represent both users and movies. If a user likes “Inception” and “Interstellar,” the model can recommend other'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='recommendations.Example: In a movie recommendation system, embeddings can be used to represent both users and movies. If a user likes “Inception” and “Interstellar,” the model can recommend other movies with similar embeddings, such as “The Matrix.”Question Answering SystemsEmbedding models power question answering (QA) systems by mapping both questions and potential answers to a common vector space. This allows the model to retrieve the most relevant answer based on the similarity between the question and the answer embeddings.Example: In a QA system, a question like “What is the capital of France?” can be embedded as a vector. The model retrieves the most similar vector from a database of potential answers, returning “Paris.”Language TranslationIn machine translation, embedding models are used to represent words and sentences in different languages as vectors in a shared embedding space. This enables models to translate between languages by finding the closest matching'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='models are used to represent words and sentences in different languages as vectors in a shared embedding space. This enables models to translate between languages by finding the closest matching vectors.Example: In a translation model, the English word “cat” and the Spanish word “gato” would have similar embeddings, allowing the model to translate between the two languages.Language Translation GPT architectureBest Practices and ConsiderationsChoosing the Right ModelTask-Specific Models: Choose the embedding model based on your specific task. For example, BERT is ideal for tasks requiring deep contextual understanding, while GPT is better suited for generative tasks.Pre-trained vs. Custom Models: Leveraging pre-trained models can save time and resources. However, fine-tuning or training custom models may be necessary for domain-specific tasks.Fine-Tuning for Domain-Specific TasksFine-tuning embedding models on domain-specific data can significantly improve performance. For instance,'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='custom models may be necessary for domain-specific tasks.Fine-Tuning for Domain-Specific TasksFine-tuning embedding models on domain-specific data can significantly improve performance. For instance, fine-tuning BERT on legal documents for a legal QA system will yield better results than using a general-purpose BERT model.Computational ConsiderationsEmbedding models, especially large ones like BERT and GPT, require substantial computational resources. It’s important to consider the trade-off between model size, training time, and inference speed when deploying embedding models in production systems.Ethical ConsiderationsEmbedding models can inadvertently capture biases present in the training data. It’s crucial to monitor and mitigate these biases to ensure that AI systems based on embedding models are fair and ethical.Example: A word embedding model might associate words like “nurse” more closely with female pronouns and “engineer” with male pronouns, reflecting societal biases.'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='models are fair and ethical.Example: A word embedding model might associate words like “nurse” more closely with female pronouns and “engineer” with male pronouns, reflecting societal biases. These biases must be addressed to prevent biased outcomes in AI applications.Recent Advancements in Embedding ModelsRoBERTa: A variant of BERT, RoBERTa optimizes training by removing the NSP task and training on larger datasets. It has demonstrated superior performance on various NLP benchmarks.GPT-4: The latest iteration of the GPT series, GPT-4, has further improved the model’s generative capabilities, making it more versatile in creative and complex text generation tasks.Sentence-BERT (SBERT): An extension of BERT, SBERT fine-tunes BERT for sentence similarity tasks by using Siamese and triplet networks. It produces embeddings that are more suitable for tasks like semantic search and clustering.Ethical ConsiderationsEmbedding models, despite their numerous applications, are not immune to'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='networks. It produces embeddings that are more suitable for tasks like semantic search and clustering.Ethical ConsiderationsEmbedding models, despite their numerous applications, are not immune to ethical concerns. These models can capture and amplify biases present in the training data, leading to unfair or discriminatory outcomes in AI systems. Recognizing and mitigating these biases is critical to developing ethical AI solutions.Bias in Embeddings: A common ethical issue in embedding models is the presence of gender, racial, or cultural biases. For example, word embeddings trained on large text corpora may associate words like “doctor” with male pronouns and “nurse” with female pronouns, reflecting societal stereotypes. These biases can perpetuate discrimination when embedding models are deployed in real-world applications like hiring systems or recommendation algorithms.Mitigating Bias: To mitigate bias, developers can employ techniques such as de biasing embeddings, where biased'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='deployed in real-world applications like hiring systems or recommendation algorithms.Mitigating Bias: To mitigate bias, developers can employ techniques such as de biasing embeddings, where biased dimensions in the embedding space are removed or neutralized. Regular audits of AI systems for bias and fairness, as well as using diverse and representative training data, can also help reduce bias in embedding models.Transparency and Accountability: Developers should also prioritize transparency and accountability in the deployment of embedding models. This includes making the training data and model decisions transparent to users, and ensuring there are mechanisms in place to address potential harms or biases that arise from the use of AI systems.Embedding models have transformative potential, but developers must approach their use with an ethical mindset, ensuring that AI systems serve all users fairly and responsibly.Embedding Models Usage in Large Language Models (LLMs)Embedding models'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='but developers must approach their use with an ethical mindset, ensuring that AI systems serve all users fairly and responsibly.Embedding Models Usage in Large Language Models (LLMs)Embedding models form the backbone of large language models (LLMs) like GPT, BERT, and their variants. These models have gained prominence in various AI applications due to their ability to generate and understand human-like text. The success of LLMs is largely due to the use of sophisticated embedding techniques that transform text into meaningful vector representations.Tokenization and Embeddings:In LLMs, the first step is to tokenize the input text, breaking it down into words, subwords, or characters. Each token is then converted into an embedding, which serves as the input to the model. These embeddings capture the semantic meaning of the tokens and serve as the foundation for further processing in the model’s layers.Self-Attention Mechanism:LLMs rely heavily on the **self-attention mechanism**, which'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='the semantic meaning of the tokens and serve as the foundation for further processing in the model’s layers.Self-Attention Mechanism:LLMs rely heavily on the **self-attention mechanism**, which allows the model to focus on different parts of the input text when generating embeddings. This mechanism enables the model to capture long-range dependencies in the text, making LLMs particularly effective in tasks that require understanding context and relationships between words.Contextualized Embeddings:Unlike traditional word embeddings, which generate a fixed vector for each word, LLMs produce **contextualized embeddings**. This means that the same word can have different embeddings depending on the surrounding context. For example, the word “bat” will have different embeddings in the sentences “The bat flew at night” and “He swung the bat.”Transformer Architecture:The transformer architecture is central to LLMs. Transformers use layers of attention mechanisms to process the input text'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='“The bat flew at night” and “He swung the bat.”Transformer Architecture:The transformer architecture is central to LLMs. Transformers use layers of attention mechanisms to process the input text and generate contextualized embeddings. Each layer refines the embeddings based on the relationships between tokens, allowing the model to build a deep understanding of the input text. In models like GPT, this process is unidirectional, with the model generating text by predicting the next word in the sequence. In BERT, the process is bidirectional, allowing the model to consider both preceding and following text when generating embeddings.Transfer Learning in LLMs:One of the key advantages of LLMs is their ability to transfer knowledge learned from large-scale pre-training tasks to specific downstream tasks. During pre-training, the model learns to generate embeddings that capture general language patterns. Fine-tuning on specific tasks, such as sentiment analysis or text classification,'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='tasks. During pre-training, the model learns to generate embeddings that capture general language patterns. Fine-tuning on specific tasks, such as sentiment analysis or text classification, allows the model to adapt these embeddings to the new task, often with minimal training data.Example: GPT-3, for instance, can be fine-tuned on a dataset of customer service conversations. The model will use its pre-trained embeddings to understand the context and generate appropriate responses, despite not being explicitly trained on customer service data during pretraining.Challenges with Embedding Models in LLMs:While embedding models in LLMs have advanced AI capabilities, they also present challenges. LLMs require vast amounts of computational resources for training and inference, making them expensive and energy-intensive. Additionally, the embeddings generated by LLMs are not always interpretable, posing challenges for transparency and trust in AI systems.Embedding models are integral to the'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='and energy-intensive. Additionally, the embeddings generated by LLMs are not always interpretable, posing challenges for transparency and trust in AI systems.Embedding models are integral to the success of LLMs, driving advancements in AI applications like text generation, translation, and question answering. As these models continue to evolve, embedding techniques will remain at the forefront of AI innovation.Additional ResourcesTo deepen your understanding of embedding models, consider the following resources:Courses“Deep Learning Specialization” by Andrew Ng on Coursera“Transformers for NLP” on UdemyBooks“Deep Learning with Python” by François Chollet“Natural Language Processing with Transformers” by Lewis Tunstall, Leandro von Werra, and Thomas WolfConclusionEmbedding models are the backbone of modern AI, enabling machines to understand and process complex data in ways that were previously impossible. From word embeddings like Word2Vec and GloVe to state-of-the-art models like'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='backbone of modern AI, enabling machines to understand and process complex data in ways that were previously impossible. From word embeddings like Word2Vec and GloVe to state-of-the-art models like BERT and GPT, embeddings have opened up new possibilities in fields such as NLP, computer vision, and recommendation systems.As these models continue to evolve, their impact on AI will only grow. By understanding and implementing embedding models, practitioners can build powerful AI systems that excel in tasks ranging from sentiment analysis to creative writing.By mastering embedding models, you’ll be well-equipped to tackle the challenges and opportunities of modern AI.NLPEmbedding ModelArtificial IntelligenceMachine LearningGui̇de----FollowWritten by Nayab Hassan28 Followers·20 FollowingHello there, I am an aspiring LLM Inference engineer who is also trying to set up his own game studio. Follow me as I discover different aspects of the IT.FollowNo responses'),\n",
       " Document(metadata={'source': 'https://medium.com/@nay1228/embedding-models-a-comprehensive-guide-for-beginners-to-experts-0cfc11d449f1', 'title': 'Embedding Models in NLP: A Comprehensive Guide to Word Embeddings and Contextualized Embeddings | Medium', 'description': \"Unlock NLP's potential with embedding models. Learn word embeddings, contextualized embeddings & applications in this comprehensive guide. Discover latest advancements & best practices for NLP success.\", 'language': 'en'}, page_content='Followers·20 FollowingHello there, I am an aspiring LLM Inference engineer who is also trying to set up his own game studio. Follow me as I discover different aspects of the IT.FollowNo responses yetHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splitted_docs = text_splitter.split_documents(docs)\n",
    "splitted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(model='nomic-embed-text', base_url=None, client_kwargs={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\"\n",
    ")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "[-0.017412899, 0.06708703, -0.15023357, -0.05632947, 0.036264837, 0.019193629, 0.03375688, -0.059481304, -0.03403214, -0.008697594, 0.008131258, 0.018587276, 0.0598532, 0.022514343, 0.008631671, -0.02986903, -0.0054631503, -0.027456393, 0.009148193, -0.035039175, 0.053598594, -0.06910708, 0.0064379345, -0.026806055, 0.117332414, 0.0051156934, -0.025523089, 0.06682302, -0.02318635, 0.0035987017, -0.017471455, 0.0022136627, -0.014296075, 0.017782269, -0.013521295, -0.006293278, -0.011610856, 0.014269431, -0.011021798, 0.050317764, -0.04216571, -0.029573282, 0.018949155, -0.0051880837, 0.017936803, 0.059473094, 0.042765953, 0.04228009, 0.06395864, -0.02426669, 0.019144686, -0.0012434566, -0.040583935, -0.07395112, 0.035632014, -0.014078738, -0.022595717, 0.007345611, -0.02302503, -0.0413428, 0.081223585, 0.082037576, -0.028611874, 0.078813106, 0.022319725, -0.048317377, -0.030788502, 0.018957509, 0.015464419, 0.023253947, 0.0041302335, -0.0075879036, -0.0069300705, 0.0061225505, -0.011965517, -0.043025136, -0.01240961, -0.018733487, -0.05355685, 0.048727497, 0.013116194, -0.04772955, 0.047845904, 0.015119457, 0.04389542, -0.0047811214, -0.050871868, -0.010766632, -0.0427304, 0.073573306, 0.06819895, 0.04951242, 0.012845391, -0.03478334, -0.09203815, -0.008669464, -0.037099738, -0.0022003774, -0.019769657, -0.014068599, -0.04610426, 0.032861404, -0.026498964, -0.024522021, 0.014677456, -0.0032063674, -0.017443145, 0.02528716, -0.015944794, 0.012442713, 0.009720838, 0.025892343, 0.046896126, -0.008290363, 0.01635249, -0.045474026, 0.027218997, 0.014701364, -0.0077134506, -0.0025909971, -0.07146666, -0.043959673, -0.011012406, 0.036566943, 0.060116246, 0.010567999, -0.06674514, 0.04122435, -0.038341515, -0.10631943, 0.018219478, -0.0032528436, -0.0021363525, -0.008318339, 0.03302086, 0.071274854, -0.047344517, -0.03038865, 0.009097233, 0.03728878, -0.026226493, 0.018443273, 0.037409205, -0.02352851, 0.029669896, 0.0024112207, -0.0015084571, 0.013836972, -0.07403709, 0.026326407, -0.028141052, 0.0045176074, 0.013046461, 0.0035954854, 0.07937951, -0.023853937, -0.018807245, 0.02435126, 0.026855158, -0.008269732, 0.06405483, -0.032096434, 0.007921181, 0.09820291, -0.034047082, -0.0071800426, -0.0004584842, 0.044717982, 0.03482069, 0.06174329, -0.030300556, 0.036441926, -0.018003237, -0.0040407716, -0.0061146696, -0.043553043, 0.040124014, -0.03963158, 0.08325142, -0.05944593, -0.008987313, -0.05752051, -0.00022168129, 0.003835264, -0.03919026, -0.017641494, -0.012689488, -0.025021898, -0.037697103, -0.043069225, -0.017288227, 0.0039882422, -0.044759892, -0.031184718, -0.034000006, -0.019397495, 0.039033107, -0.024070587, 0.060244888, -0.035843547, -0.008026533, -0.0085327, -0.03763305, 0.027781151, -0.050456733, -0.04249022, -0.0015029687, 0.007992754, -0.03945049, 0.018658979, 0.04467921, -0.011647032, 0.013876221, -0.0095226215, 0.02662341, 0.00030686162, -0.009521032, -0.07864921, -0.025322514, 0.0072281267, 0.05073866, -0.007509409, 0.070340596, -0.018912038, 0.009826772, -0.02461589, -0.030004427, -0.024263771, 0.013517391, 0.034026973, 0.038647797, -0.05309357, 0.031441506, -0.027377319, 0.01649979, 0.065909915, -0.019339535, 0.065021746, -0.02097809, 0.01160167, 0.030682866, 0.058331072, 0.008603316, -0.0056907493, -0.04826577, 0.015427628, -0.003239992, -0.015029826, -0.006211443, 0.009156279, -0.015340996, -0.055477135, -0.015818993, 0.02809148, 0.0017768196, -0.07123595, 0.011355301, -0.03541456, 0.015489268, -0.05981274, -0.012538298, -0.06376985, 0.05924012, 0.028912496, 0.031568248, 0.0049627624, 0.028213628, -0.018485337, 0.009216332, 0.012411896, 0.043492906, 9.0453184e-05, -0.0062038167, 0.02144257, 0.003171744, 0.006176921, 0.015310684, 0.020282743, -0.0026887243, 0.026363745, -0.03790549, -0.057355944, -0.071088694, 0.007549573, 0.023235586, 0.03519728, 0.013137025, 0.020688254, -0.0013528698, -0.0036545522, -0.0058531445, -0.04270033, -0.04008338, -0.009425709, -0.010566556, -0.0074933036, 0.029756535, -0.010989864, -0.011516371, 0.00734524, 0.003036628, -0.0066730045, 0.0656266, 0.015317901, -0.0010700059, -0.04734296, 0.014091839, 0.009614418, 0.0077086394, -0.0244276, -0.07054343, -0.0089576505, 0.040972624, -0.005327449, -0.011024983, 0.035659257, 0.0059038573, -0.00389194, 0.028100261, -0.01812245, 0.014323242, -0.08625188, -0.028127287, 0.002743747, -0.02677037, 0.084647946, -0.047706924, 0.06295917, -0.030933473, -0.0073603923, 0.04820108, 0.033535782, -0.010176142, -0.020515889, 0.014140342, 0.041130755, -0.016383514, 0.06743487, -0.0040038163, 0.018253634, 0.045614313, -0.01323291, 0.043238256, -0.028112395, -0.06408335, -0.027214399, -0.007552248, 0.029010713, -0.039598655, 0.038969655, -0.018514117, 0.03006726, -0.0038727715, -0.03871717, 0.008223641, -0.017305635, 0.024792612, 0.06142832, -0.0005547227, -0.004295344, 0.03966584, 0.023206552, 0.04294457, 0.015849734, 0.024178753, 0.022954155, -0.011872654, 0.019804055, 0.042033043, -0.044421677, -0.012318241, -0.0244758, 0.020250097, 0.07361639, -0.006041444, 0.03280823, -0.0768749, -0.015664402, 0.008846672, -0.0017128241, -0.018343862, -0.00831329, 0.023358708, 0.043370917, 0.0034961493, -0.033917286, 0.009198003, 0.0002808012, 0.030025935, 0.017683005, -0.012335448, -0.029966047, -0.037442062, 0.02481018, -0.0017424541, 0.021977725, -0.016391193, 0.0017606809, -0.0022746446, 0.045865156, -0.060771216, -0.010621788, 0.034462847, -0.022133622, 0.0013029664, 0.032684445, -0.027600698, 0.0020232743, 0.05234917, -0.033463106, 0.03307895, 0.022677729, -0.0010651924, -0.048324443, 0.019844636, 0.018576303, 0.07596553, 0.06494404, -0.026835177, -0.032138832, 0.0063294056, 0.036705445, 0.04809795, -0.029945623, 0.023842312, -0.016126292, -0.027366545, 0.06675237, 0.0057233, -0.0868282, -0.013731825, 0.0046390495, 0.0041405163, -0.055204365, 0.0054932004, 0.07939777, 0.04366639, 0.027191212, 0.03151118, -0.018773286, 0.09061056, -0.021271456, -0.014045155, -0.070038445, -0.00482202, 0.10195132, 0.027854284, -0.052852284, -0.033078462, 0.023334838, 0.066562176, -0.014476905, 0.04111575, 0.009586299, 0.057900865, -0.03433528, 0.004123215, -0.00234953, -0.0081843035, 0.044699643, 0.013997081, 0.00600898, -0.05327243, 0.02168332, 0.017430466, -0.039650574, 0.03619696, -0.027788099, 0.020198064, 0.05050712, -0.021133572, -0.050473806, 0.03987895, -0.036108956, -0.019902695, 0.011492121, 0.00019160415, -0.03235919, 0.018656654, 0.023425218, -0.014936181, -0.05306687, -0.059357706, -0.028433366, -0.016623644, 0.0077091325, 0.013958991, 0.017303273, 0.0025719802, -0.0040477617, 0.05102526, 0.0035940593, 0.027929151, -0.016391749, -0.00910781, -0.038778044, -0.023071112, 0.025589628, 0.017458046, -0.018907497, 0.021082709, -0.021049421, -0.0011908755, 0.029944308, -0.014100261, -0.0057209805, -0.0022892312, -0.034743704, -0.031199232, 0.03497749, 0.011103091, 0.017595427, 0.062263727, -0.0029068622, 0.025879018, -0.021462025, 0.05496119, 0.038930204, -0.07488558, 0.020378757, 0.018529624, -0.01683114, 0.06775368, -0.00023615728, -0.071473695, 0.043026056, 0.026762675, -0.04453867, 0.01683518, -0.013281746, -0.03810565, 0.018870508, -0.019098977, -0.06399612, 0.023129454, 0.013583401, -0.007928422, -0.00013164488, 0.037422504, 0.028441032, 0.0520219, 0.011035786, 0.007346727, -0.044872463, -0.024395859, -0.005739524, -0.09248227, 0.04753132, 0.015315976, -0.060903955, 0.045521677, -0.07603737, 0.007745488, 0.011494545, -0.0007270554, 0.0072726905, -0.014746224, -0.022697369, 0.007579673, 0.026820999, -0.0072006118, -0.0040058107, 0.07100311, -0.028305791, 0.042441797, -0.033982035, 0.01724118, 0.0328336, -0.002349175, 0.050166797, -0.03258011, -0.04246565, -0.0073162355, 0.015387201, 0.03804071, -0.0106370635, 0.02767783, -0.031465996, -0.056428175, 0.00045311902, 0.05503685, -0.06991326, 0.019858746, 0.011038505, -0.007612668, -0.016883003, -0.05797042, 0.0056880475, 0.011950057, -0.08206459, -0.017577723, 0.065168135, 0.03623489, -0.0076449243, -0.026654474, -0.01528241, -0.056543227, -0.01592127, 0.0010071883, -0.06731244, 5.8107726e-05, -0.0040647173, 0.028521506, 0.023074863, -0.009869006, 0.041515853, 0.016664457, -0.0017683188, -0.012731446, -0.055847306, -0.0034129967, 0.0033260854, -0.026842436, 0.02437106, 0.03618407, -0.047100104, 0.009259216, -0.07404519, 0.031917922, -0.016670462, -0.073633194, -0.05817035, 0.063994415, -0.004989408, 0.031122677, -0.0070772306, -0.05455635, 0.03496418, 0.015475824, 0.0878481, -0.030943267, 0.0116048865, -0.06702891, -0.009955389, -0.048526566, 0.0004953757, -0.0034683703, 0.021393875, 0.05372431, 0.047645893, 0.01851047, -0.0023865646, -0.047287155, 0.07587329, 0.036051683, -0.040781673, 0.018895412, 0.029313495, -0.0032313033, -0.0020722926, 0.07400359, 0.062296186, 0.010531141, -0.01880383, -0.03684772, 0.00047057582, 0.015630126, -0.039866105, -0.02682117, -0.009444047, 0.008116255, -0.001827816, -0.003500712, -0.0069242874, 0.030857025, 0.0066627422, -0.004704688, -0.008577059, -0.04791118, 0.0076493286, 0.0031414328, 0.002901294, 0.0067006676, -0.061188985, 0.036617372, 0.028653327, 0.033584066, 0.005407627, -0.005965454, -0.0366847, 0.020828472, -0.028763808, -0.010040035, -0.025016377, 0.025755405, -0.032223362, 0.049278576, -0.008568593, 0.019528178, -0.058197543, -0.014592561, 0.0059226556, 0.014057984, 0.0046531637, -0.024984589, 0.0345663, 0.0053333323, -0.026784625, 0.025556933, 0.045632794, -0.016820518, 0.030397132, 0.012368803, 0.014457813, -0.0367035, 0.0011189783, -0.0017395656, -0.0071361447, -0.020279445, -0.050047424, -0.018674303, -0.00047259472, 0.0041427817, 0.10072651, -0.039136823, 0.065810025, 0.0064857383, 0.0006772048, 0.008780526, 0.00311488, 0.0661233, -0.046007097, -0.039612893, -0.01421924, -0.037468996, 0.05082514, 0.037237607, -0.07851122, 0.040719192, -0.0478054, 0.0023325167, -0.03193494, -0.030529955, 0.056031708, -0.012200786, -0.0006512672, -0.08057898, -0.06435936, -0.043589894, 0.006592479, -0.010211762, 0.04404062, -0.013120283, -0.032177217, 0.024296809, 0.05578718, -0.038848605, 0.0337805, 0.018501788, -0.031169208, 0.07981824, 0.0005518, -0.032305423, -0.01997589, -0.029326156, 0.00961688, -0.033869483, 0.05588492, 0.069012135, -0.008393837, 0.011687833, -0.036652986, -0.014082309, -0.01926572, -0.015517405, -0.06041759, -0.047787674, -0.00096621533]\n"
     ]
    }
   ],
   "source": [
    "## A temp code just to see whether the embeddings is working or not\n",
    "vector = embeddings.embed_query(\"This is a GEN AI application that uses web base loader\")\n",
    "print(len(vector))\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[1;32m----> 2\u001b[0m vectorstoredb \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43msplitted_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m vectorstoredb\n",
      "File \u001b[1;32md:\\TCS\\NextGen_3.10\\venv\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:841\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_documents\u001b[39m(\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[VST],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    830\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VST:\n\u001b[0;32m    831\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return VectorStore initialized from documents and embeddings.\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \n\u001b[0;32m    833\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;124;03m        VectorStore: VectorStore initialized from documents and embeddings.\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    842\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32md:\\TCS\\NextGen_3.10\\venv\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:841\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_documents\u001b[39m(\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[VST],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    830\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VST:\n\u001b[0;32m    831\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return VectorStore initialized from documents and embeddings.\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \n\u001b[0;32m    833\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;124;03m        VectorStore: VectorStore initialized from documents and embeddings.\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 841\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    842\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb = FAISS.from_documents(splitted_docs,embeddings)\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
